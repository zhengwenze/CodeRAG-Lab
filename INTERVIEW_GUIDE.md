# CodeRAG Lab - 面试项目介绍

## 项目概述

**CodeRAG Lab** 是一个基于 RAG（检索增强生成）技术的专业级代码库问答系统，旨在帮助开发者通过自然语言快速检索和理解代码库。

---

## 技术栈

### 后端
- **Python 3.9+**
- **FastAPI** - 高性能 Web 框架
- **Qdrant/FAISS** - 向量数据库
- **PostgreSQL + pgvector** - 混合向量存储
- **llama.cpp** - 本地 LLM 推理
- **Sentence Transformers** - 嵌入模型
- **PEFT/LoRA** - 模型微调
- **Whoosh** - 全文搜索引擎

### 前端
- **Vue3 + Vite** - 现代前端框架
- **Element Plus** - UI 组件库
- **Pinia** - 状态管理
- **Vue Router** - 路由管理

### 部署
- **Docker Compose** - 一键部署
- **GitHub Actions** - CI/CD

---

## 核心功能

### 1. 可溯源 RAG 问答
- 用户提问 → 向量检索 → 上下文拼接 → LLM 生成 → 返回带引用的答案
- 引用包含文件路径、行号、相似度分数

### 2. 混合检索架构
- **向量检索**：使用 Sentence Transformers 生成语义嵌入
- **全文检索**：使用 Whoosh/BM25 进行关键词匹配
- **结果融合**：加权合并两种检索结果

### 3. LLM Rerank
- 使用 Cross-Encoder 模型对初检结果重排序
- 提升检索精度和相关性

### 4. 多格式文档解析
- 支持 PDF、Word、Markdown、Text、CSV、JSON、YAML 等格式
- 统一解析接口，支持扩展

### 5. 完整评测体系
- 内置评测数据集
- 指标：hit_rate@k、citation_rate、recall、MRR
- 回归测试：对比历史结果，检测性能回退

### 6. LoRA 微调
- 使用 PEFT 库进行 LoRA/QLoRA 微调
- 支持基础模型与微调模型对比评测

### 7. 性能压测
- 并发压测支持
- 延迟统计：P50、P95、P99
- 资源监控：CPU、内存

### 8. 安全防护
- 输入验证：长度限制、危险字符过滤
- 输出清理：HTML 转义、XSS 防护

---

## 项目架构

```
┌─────────────────────────────────────────────────────────────┐
│                         前端 (Vue3/Next.js)                 │
└─────────────────────────────┬───────────────────────────────┘
                              │ HTTP
┌─────────────────────────────▼───────────────────────────────┐
│                      FastAPI Backend                         │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────┐ │
│  │   Chat API  │  │  Dataset API │  │   Evaluation API   │ │
│  └──────┬──────┘  └──────┬──────┘  └──────────┬──────────┘ │
└─────────┼────────────────┼────────────────────┼─────────────┘
          │                │                    │
┌─────────▼────────────────▼────────────────────▼─────────────┐
│                      RAG Pipeline                            │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────┐ │
│  │  Retrieval  │  │   Rerank    │  │   Prompt Builder   │ │
│  │ (Vector+BM25)│  │ (Cross-Enc) │  │                     │ │
│  └──────┬──────┘  └──────┬──────┘  └──────────┬──────────┘ │
└─────────┼────────────────┼────────────────────┼─────────────┘
          │                │                    │
┌─────────▼────────────────▼────────────────────▼─────────────┐
│                      Vector Store                             │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────┐ │
│  │   Qdrant    │  │    FAISS    │  │   pgvector (PG)    │ │
│  └─────────────┘  └─────────────┘  └─────────────────────┘ │
└─────────────────────────────────────────────────────────────┘
          │
┌─────────▼───────────────────────────────────────────────────┐
│                      LLM Layer                               │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────┐ │
│  │  llama.cpp  │  │  OpenAI API │  │   Ollama (Local)    │ │
│  └─────────────┘  └─────────────┘  └─────────────────────┘ │
└─────────────────────────────────────────────────────────────┘
```

---

## 关键技术亮点

### 1. 混合检索策略
```python
# 向量检索权重 + 全文检索权重
final_score = vector_score * 0.7 + bm25_score * 0.3
```

### 2. RAG Prompt 模板
```
基于以下代码片段回答用户问题：

【代码片段 1】file: src/main.py, line: 10-20
{code_content_1}

【代码片段 2】file: src/utils.py, line: 50-60
{code_content_2}

问题：{user_question}

请给出答案，并引用相关代码片段。
```

### 3. LoRA 微调配置
```python
lora_config = LoraConfig(
    r=8,  # rank
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)
```

---

## 项目亮点（面试必讲）

### 1. 工程化能力
- 完整的 Docker 一键部署
- GitHub Actions CI/CD
- 模块化设计，易于扩展

### 2. 检索效果优化
- 混合检索 + Rerank 显著提升召回率
- 评测体系帮助量化优化效果

### 3. 端到端解决方案
- 从文档入库到问答输出全链路
- 支持多种 LLM 和向量存储后端

### 4. 性能与安全
- 压测工具确保系统稳定性
- 输入输出安全防护

---

## 常见面试问题

### Q1: RAG 的核心思想是什么？
RAG（检索增强生成）通过从外部知识库检索相关文档，将检索结果作为上下文提供给 LLM，从而解决 LLM 知识过时、幻觉等问题。

### Q2: 如何提升 RAG 的检索效果？
1. **混合检索**：结合向量和全文检索
2. **重排序**：使用 Cross-Encoder 对初检结果重排
3. **优化分块**：根据语义进行智能分块
4. **评测驱动**：通过评测指标指导优化方向

### Q3: 向量检索和全文检索的区别？
- **向量检索**：基于语义相似度，适合理解意图
- **全文检索**：基于关键词匹配，适合精确匹配
- 两者结合可以互补长短

### Q4: 如何处理大文档？
1. **智能分块**：按语义单元（函数、类、段落）分块
2. **重叠窗口**：相邻块之间有重叠，避免信息丢失
3. **层级索引**：建立文档结构索引，支持多级检索

### Q5: LoRA 微调的原理？
LoRA（Low-Rank Adaptation）通过在预训练模型旁添加低秩矩阵来学习任务相关知识，避免全参数微调的高成本。

---

## 项目 GitHub

- Gitee: https://gitee.com/zwz050418/code-rag-lab
- GitHub: https://github.com/zhengwenze/CodeRAG-Lab

---

## 快速启动

```bash
# Docker 一键部署
docker-compose up -d

# 访问
# 前端: http://localhost:5173
# API: http://localhost:8000
# Qdrant: http://localhost:6333
```
